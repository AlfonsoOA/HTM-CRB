# Install and load necessary packages
# Make sure you have these packages installed. If not, uncomment the `install.packages()` lines.
# install.packages("tidyverse")
# install.packages("stringr")
# install.packages("pheatmap")
# install.packages("dunn.test")
# install.packages("ggpubr")
# install.packages("readxl") # Necessary to read .xls files

library(tidyverse)
library(stringr)
library(pheatmap)
library(dunn.test)
library(ggpubr)
library(readxl) # Load the library to read .xls

# --- GLOBAL PATHS AND LIBRARIES CONFIGURATION ---
# Define the base path for all projects
# ENSURE TO CHANGE THIS PATH TO YOUR COMPUTER'S PATH!
base_analysis_path <- "C:/Users/Alfonso/Desktop/AlfonsoOA_MSI/1Investigacion/7_Especial_IJMS/analisis27062025/ClueGO_results/"

# Path where the RAW ClueGO .xls files are located (your original files)
# EXAMPLE: If your raw files are in `ClueGO_results/Raw_ClueGO_Outputs/`
cluego_raw_folder_path <- base_analysis_path

# Path where the consolidated TSV files generated by this script will be saved
consolidated_tsvs_folder_path <- file.path(base_analysis_path, "consolidated_tsvs")

# Path where the final analysis results (matrices, heatmaps, boxplots) will be saved
output_results_folder_path <- file.path(base_analysis_path, "ClueGO_analysis")


# --- 1. Global Variables and Mappings Definition ---

# List of all expected method names
# Ensure this list is exhaustive and reflects your columns in the consolidated TSVs.
all_expected_method_names <- c(
  "bayes_Bayes", "bayes_FC", # Although bayes_FC may not exist in files, we include it for structure
  "deqms_Bayes", "deqms_FC",
  "limma_Bayes", "limma_FC",
  "msstats_Bayes", "msstats_FC",
  "tstudent_Bayes", "tstudent_FC",
  "twelch_Bayes", "twelch_FC"
)

# Definition of Hypothesis Testing Methods (HTMs) and Criteria for Biological Relevance (CBR)
htms <- c("bayes", "deqms", "limma", "msstats", "tstudent", "twelch")
cbrs <- c("Bayes", "FC")

# Classification of methods for determining comparison type
methods_FC <- c("bayes_FC", "deqms_FC", "limma_FC", "msstats_FC", "tstudent_FC", "twelch_FC")
methods_Bayes <- c("bayes_Bayes", "deqms_Bayes", "limma_Bayes", "msstats_Bayes", "tstudent_Bayes", "twelch_Bayes")

# --- ONTOLOGY MAPPING BY WORK ---
works_ontology_map <- list(
  W1 = c("GO_BiologicalProcess-EBI-UniProt-GOA-ACAP-ARAP_02.06.2025_00h00",
         "GO_MolecularFunction-EBI-UniProt-GOA-ACAP-ARAP_02.06.2025_00h00",
         "KEGG_25.05.2022"),
  W2 = c("GO_BiologicalProcess-Custom-GOA-ACAP-ARAP_04.11.2021_00h00",
         "GO_MolecularFunction-Custom-GOA-ACAP-ARAP_04.11.2021_00h00"),
  W3 = c("GO_BiologicalProcess-Custom-GOA-ACAP-ARAP_30.05.2022_00h00",
         "GO_MolecularFunction-Custom-GOA-ACAP-ARAP_30.05.2022_00h00",
         "KEGG_30.05.2022"),
  W4 = c("GO_BiologicalProcess-Custom-GOA_24.07.2019_00h00",
         "GO_MolecularFunction-Custom-GOA_24.07.2019_00h00"),
  W5 = c("GO_BiologicalProcess-Custom-GOA_24.07.2019_00h00",
         "GO_MolecularFunction-Custom-GOA_24.07.2019_00h00")
)

# List of work names (e.g., "W1", "W2", ...)
works_list <- names(works_ontology_map)


# --- 2. Auxiliary Functions ---

# AUXILIARY FUNCTION TO EXTRACT THE BASE METHOD NAME (HTM)
get_htm_name <- function(method_name) {
  str_split(method_name, "_")[[1]][1]
}

# AUXILIARY FUNCTION TO EXTRACT THE CBR TYPE (Bayes/FC)
get_cbr_type <- function(method_name) {
  str_split(method_name, "_")[[1]][2]
}

# FUNCTION TO CLASSIFY THE COMPARISON TYPE
get_comparison_type <- function(method1, method2, methods_FC_list, methods_Bayes_list) {
  is_m1_fc <- method1 %in% methods_FC_list
  is_m1_bayes <- method1 %in% methods_Bayes_list
  is_m2_fc <- method2 %in% methods_FC_list
  is_m2_bayes <- method2 %in% methods_Bayes_list
  
  htm1 <- get_htm_name(method1)
  htm2 <- get_htm_name(method2)
  
  # Intra-HTM_FC_CBR: Comparisons between different HTMs where the CBR was consistently Fold Change-based
  if (is_m1_fc && is_m2_fc) {
    return("Intra-HTM_FC_CBR")
  }
  # Intra-HTM_Bayes_CBR: Comparisons between different HTMs where the CBR was consistently Bayesian posterior probability-based
  else if (is_m1_bayes && is_m2_bayes) {
    return("Intra-HTM_Bayes_CBR")
  }
  # Intra-CBR_Fixed_HTM: Comparisons between the two different CBRs (FC vs Bayes) where the HTM was kept constant
  else if ((is_m1_fc && is_m2_bayes || is_m1_bayes && is_m2_fc) && (htm1 == htm2)) {
    return("Intra-CBR_Fixed_HTM")
  }
  # Inter-HTM_Inter-CBR: Comparisons between combinations where both the HTM and the CBR differed
  else if ((is_m1_fc && is_m2_bayes || is_m1_bayes && is_m2_fc) && (htm1 != htm2)) {
    return("Inter-HTM_Inter-CBR")
  }
  # Any other unclassified combination
  else {
    return("Unknown_Comparison")
  }
}

# AUXILIARY FUNCTION TO CALCULATE JACCARD INDEX
calculate_jaccard_index <- function(set1, set2) {
  if (length(set1) == 0 && length(set2) == 0) {
    return(0)
  }
  
  intersection_size <- length(intersect(set1, set2))
  union_size <- length(union(set1, set2))
  
  if (union_size == 0) {
    return(0)
  } else {
    return(intersection_size / union_size)
  }
}

# AUXILIARY FUNCTION TO CALCULATE EUCLIDEAN DISTANCE AND TRANSFORM IT INTO SIMILARITY
calculate_euclidean_similarity <- function(vec1, vec2) {
  # Ensure vectors have the same length and contain only numeric values
  if (length(vec1) != length(vec2) || !is.numeric(vec1) || !is.numeric(vec2)) {
    stop("Vectors for Euclidean distance must be numeric and have the same length.")
  }
  distance <- dist(rbind(vec1, vec2), method = "euclidean")
  # Transform distance to similarity (0 to 1). Distance 0 -> Similarity 1. Infinite distance -> Similarity 0.
  similarity <- 1 / (1 + as.numeric(distance))
  return(similarity)
}


# FUNCTION TO CONSOLIDATE INDIVIDUAL CLUEGO RESULTS (.xls) INTO A SINGLE TSV PER WORK AND DIRECTION
consolidate_cluego_results <- function(base_input_dir, output_tsv_dir, htms, cbrs, all_expected_method_names, works_list) {
  cat("\n--- INITIATING CLUEGO (.xls) FILE CONSOLIDATION ---\n")
  
  # Ensure the output directory for consolidated TSVs exists
  if (!dir.exists(output_tsv_dir)) {
    dir.create(output_tsv_dir, recursive = TRUE)
    cat(paste("Directory for consolidated TSVs created at:", output_tsv_dir, "\n"))
  }
  
  for (work_name in works_list) {
    for (direction_label in c("up", "down")) {
      cat(paste0("\nConsolidating ", work_name, " - Direction: ", direction_label, "...\n"))
      
      # Initialize an empty dataframe to consolidate results for this work and direction
      consolidated_df <- tibble(
        Ontology = character(),
        Term = character()
      )
      
      # Loop for each HTM and CBR combination
      for (htm in htms) {
        for (cbr in cbrs) {
          method_col_name <- paste0(htm, "_", cbr)
          
          # The "bayes_FC" methods do not exist in the user's classification.
          # We ignore the "bayes_FC" combination to avoid searching for a non-existent file.
          if (htm == "bayes" && cbr == "FC") {
            next
          }
          
          # Construct the expected file name. Ensure it matches your files.
          # EXAMPLE: W1_deqms_Bayes_ClueGO_down.xls
          file_name <- paste0(work_name, "_", htm, "_", cbr, "_ClueGO_", direction_label, ".xls")
          file_path <- file.path(base_input_dir, file_name)
          
          if (file.exists(file_path)) {
            cat(paste0("  Reading: ", file_name, "\n"))
            df_current <- tryCatch({
              read_excel(file_path) %>%
                # Use "Ontology Source" to map to "Ontology"
                select(Term = "Term", Ontology = "Ontology Source", `Associated Genes` = "% Associated Genes") %>%
                # Convert to tibble for tidyverse compatibility
                as_tibble()
            }, error = function(e) {
              cat(paste("  WARNING: Could not read file", file_name, ":", e$message, ". It will be skipped.\n"))
              return(NULL)
            })
            
            if (!is.null(df_current) && nrow(df_current) > 0) {
              # NEW STEP: Group by Ontology and Term and calculate the mean of '% Associated Genes'
              # to handle potential duplicates within the same ClueGO file
              df_current <- df_current %>%
                group_by(Ontology, Term) %>%
                summarise(`Associated Genes` = mean(`Associated Genes`, na.rm = TRUE), .groups = 'drop')
              
              # Rename the value column to match the method name
              df_current <- df_current %>%
                rename(!!sym(method_col_name) := `Associated Genes`) %>%
                # Select only the necessary columns for joining
                select(Ontology, Term, !!sym(method_col_name))
              
              # Perform the join with the consolidated dataframe.
              # Use full_join to keep all terms and ontologies, filling with NA where no data exists.
              if (nrow(consolidated_df) == 0) {
                # If it's the first method, initialize the consolidated dataframe
                consolidated_df <- df_current
              } else {
                # If it's not the first method, join by Ontology and Term
                consolidated_df <- full_join(consolidated_df, df_current, by = c("Ontology", "Term"))
              }
            } else {
              cat(paste0("  INFO: File ", file_name, " exists but is empty or has no valid data. It will be treated as having no data.\n"))
            }
            
          } else {
            cat(paste0("  WARNING: File not found: ", file_name, ". Assuming no data for this method/filter.\n"))
          }
        } # End CBR loop
      } # End HTM loop
      
      # Fill NAs with 0 for methods that had no data (either because the file was not found or no enriched terms)
      # This is crucial so that 0 means "not enriched" in Jaccard calculation and for correlations.
      for (col_name in all_expected_method_names) {
        if (!col_name %in% colnames(consolidated_df)) {
          consolidated_df[[col_name]] <- 0
        }
        consolidated_df[[col_name]][is.na(consolidated_df[[col_name]])] <- 0
      }
      
      # Ensure columns are in the expected order (Ontology, Term, then methods)
      consolidated_df <- consolidated_df %>%
        select(Ontology, Term, all_of(all_expected_method_names))
      
      # Save the consolidated DataFrame in TSV format
      output_tsv_name <- paste0(work_name, "_0_Enrich_comp_results_", direction_label, "_all_methods.tsv")
      output_tsv_path <- file.path(output_tsv_dir, output_tsv_name)
      write_tsv(consolidated_df, output_tsv_path)
      cat(paste0("  Consolidated TSV saved for ", work_name, " (", direction_label, ") at: ", output_tsv_path, "\n"))
    } # End direction loop
  } # End work_name loop
  
  cat("\n--- CLUEGO (.xls) FILE CONSOLIDATION COMPLETED ---\n")
}


# --- 3. Main Function to Process a TSV File and Perform Analysis ---

process_consolidated_tsv_for_work_analysis <- function(
    tsv_file_path, work_name, direction_label,
    output_base_dir,
    all_expected_method_names, methods_FC_list, methods_Bayes_list
) {
  
  cat(paste0("\n--- Processing TSV file: ", basename(tsv_file_path), " (", work_name, ", ", direction_label, ") ---\n"))
  
  if (!file.exists(tsv_file_path)) {
    cat(paste("WARNING: TSV file not found:", tsv_file_path, ". Skipping.\n"))
    return(tibble()) # Returns an empty tibble if the file does not exist
  }
  
  df_consolidated <- tryCatch({
    read_tsv(tsv_file_path, show_col_types = FALSE) # show_col_types = FALSE to clean output
  }, error = function(e) {
    cat(paste("ERROR: Could not read TSV file", basename(tsv_file_path), ":", e$message, "\n"))
    return(NULL)
  })
  
  if (is.null(df_consolidated) || nrow(df_consolidated) == 0) {
    cat(paste("INFO: File", basename(tsv_file_path), "is empty or could not be processed. Skipping.\n"))
    return(tibble())
  }
  
  # Ensure method columns are numeric and handle NAs.
  # Also add columns with 0 if any expected method is not in the TSV.
  for (col in all_expected_method_names) {
    if (col %in% colnames(df_consolidated)) {
      df_consolidated[[col]] <- as.numeric(df_consolidated[[col]])
      df_consolidated[[col]][is.na(df_consolidated[[col]])] <- 0
    } else {
      df_consolidated[[col]] <- 0
      cat(paste0("INFO: Method column '", col, "' not found in TSV for ", work_name, " and was added with zeros.\n"))
    }
  }
  
  all_ontologies_in_tsv <- unique(df_consolidated$Ontology)
  
  # Output paths updated for the current work (now within output_base_dir)
  output_folder_current_work_path <- file.path(output_base_dir, paste0(work_name, "_", direction_label, "_Results"))
  
  # Creation of the base folder for this work and direction
  if (!dir.exists(output_folder_current_work_path)) {
    dir.create(output_folder_current_work_path, recursive = TRUE)
    cat(paste("Results folder for ", work_name, " (", direction_label, ") created at:", output_folder_current_work_path, "\n"))
  }
  
  # Initialize tibbles to collect data for each metric
  all_metrics_data_for_current_work_direction <- tibble(
    Work = character(),
    Ontology = character(),
    Direction = character(),
    Metric_Type = character(), # Jaccard, Pearson, Spearman, Euclidean
    Value = numeric(),
    Transformed_Value = numeric(), # For Jaccard (arcsin(sqrt)), and Euclidean (1/(1+d))
    Comparison_Type = character()
  )
  
  for (ontology_name_raw in all_ontologies_in_tsv) {
    # Normalize ontology name for file names and titles
    ontology_name_for_file <- str_replace_all(ontology_name_raw, "[^[:alnum:]]", "_")
    
    # Try to clean the name for the plot, removing date/etc suffixes.
    ontology_name_for_plot <- str_replace(ontology_name_raw, "-EBI-UniProt-GOA-ACAP-ARAP_\\d{2}\\.\\d{2}\\.\\d{4}_\\d{2}h\\d{2}$", "")
    ontology_name_for_plot <- str_replace(ontology_name_for_plot, "_\\d{2}\\.\\d{2}\\.\\d{4}$", "")
    ontology_name_for_plot <- str_replace(ontology_name_for_plot, "_Custom_GOA(_\\d{2}\\.\\d{2}\\.\\d{4}_\\d{2}h\\d{2})?$", "")
    
    df_ontology_current <- df_consolidated %>% filter(Ontology == ontology_name_raw)
    
    # --- Data Preparation for Jaccard, Pearson, Spearman, Euclidean ---
    # Get all unique terms for this ontology across all methods.
    all_terms_in_ontology <- unique(df_ontology_current$Term)
    
    # For Jaccard: Sets of terms with values > 0
    enriched_terms_by_method_jaccard <- list()
    # For correlations and euclidean: Numeric vectors with actual values (or 0 if not enriched)
    numeric_values_by_method <- list()
    
    for (method_col in all_expected_method_names) {
      # Jaccard: only terms > 0
      terms_for_jaccard <- df_ontology_current %>%
        filter(!!sym(method_col) > 0) %>%
        pull(Term) %>%
        unique()
      # If no terms >0, explicitly assign an empty set.
      enriched_terms_by_method_jaccard[[method_col]] <- if (length(terms_for_jaccard) > 0) terms_for_jaccard else character(0)
      
      # Numeric: create an aligned vector for all terms in the universe
      numeric_vec <- numeric(length(all_terms_in_ontology))
      names(numeric_vec) <- all_terms_in_ontology
      
      # Fill the vector with method values for present terms
      current_method_data <- df_ontology_current %>%
        select(Term, !!sym(method_col)) %>%
        distinct(Term, .keep_all = TRUE) # Ensures each term is unique
      
      # Assign numeric values. Terms not found in current_method_data
      # (because their value was 0 or original NA) are already initialized to 0 in numeric_vec.
      for (term_idx in seq_along(all_terms_in_ontology)) {
        term <- all_terms_in_ontology[term_idx]
        val <- current_method_data %>% filter(Term == term) %>% pull(!!sym(method_col))
        if (length(val) > 0) {
          numeric_vec[term] <- val[1] # Take the first value if there are duplicates (shouldn't be after distinct)
        }
      }
      numeric_values_by_method[[method_col]] <- numeric_vec
      
      # Debugging for methods with no enriched terms or all zeros
      if (length(terms_for_jaccard) == 0) {
        cat(paste0("  DEBUG: Ontology '", ontology_name_for_plot, "': Method '", method_col, "' has NO enriched terms (>0) for Jaccard (set as empty).\n"))
      }
      if (all(numeric_vec == 0) && length(numeric_vec) > 0) {
        cat(paste0("  DEBUG: Ontology '", ontology_name_for_plot, "': Method '", method_col, "' has ALL numeric values as 0 for correlation/Euclidean.\n"))
      }
    }
    
    # Ensure we have at least 2 expected methods for comparison
    if (length(all_expected_method_names) < 2) {
      cat(paste("WARNING: Fewer than 2 methods defined for comparison in all_expected_method_names. Skipping for", ontology_name_for_plot, ".\n"))
      next
    }
    
    n_actual_methods <- length(all_expected_method_names)
    
    # --- Calculate and Store Jaccard Index ---
    jaccard_matrix <- matrix(NA, nrow = n_actual_methods, ncol = n_actual_methods)
    rownames(jaccard_matrix) <- all_expected_method_names
    colnames(jaccard_matrix) <- all_expected_method_names
    
    for (i in 1:n_actual_methods) {
      for (j in 1:n_actual_methods) {
        m1_name <- all_expected_method_names[i]
        m2_name <- all_expected_method_names[j]
        
        if (i == j) {
          jaccard_matrix[i, j] <- 1
        } else {
          jaccard_val <- calculate_jaccard_index(enriched_terms_by_method_jaccard[[m1_name]], enriched_terms_by_method_jaccard[[m2_name]])
          jaccard_matrix[i, j] <- jaccard_val
          
          # Collect data for the tibble
          if (j > i) { # Only upper diagonal to avoid duplicates
            comp_type <- get_comparison_type(m1_name, m2_name, methods_FC_list, methods_Bayes_list)
            all_metrics_data_for_current_work_direction <- all_metrics_data_for_current_work_direction %>%
              add_row(
                Work = work_name,
                Ontology = ontology_name_for_plot,
                Direction = direction_label,
                Metric_Type = "Jaccard",
                Value = jaccard_val,
                Transformed_Value = asin(sqrt(jaccard_val)), # Transformation for normality for parametric tests
                Comparison_Type = comp_type
              )
          }
        }
      }
    }
    cat(paste("Jaccard matrix for ontology:", ontology_name_for_plot, " (", direction_label, ") - ", work_name, ":\n"))
    print(round(jaccard_matrix, 3))
    # Save the Jaccard matrix to a text file
    jaccard_txt_path <- file.path(output_folder_current_work_path,
                                  paste0(work_name, "_jaccard_matrix_",
                                         ontology_name_for_file, "_", direction_label, ".txt"))
    write.table(jaccard_matrix, file = jaccard_txt_path, sep = "\t", quote = FALSE, row.names = TRUE, col.names = NA)
    cat(paste("Jaccard matrix for", ontology_name_for_plot, " (", direction_label, ") saved at:", jaccard_txt_path, "\n"))
    
    # --- Calculate and Store Pearson Correlation ---
    pearson_matrix <- matrix(NA, nrow = n_actual_methods, ncol = n_actual_methods)
    rownames(pearson_matrix) <- all_expected_method_names
    colnames(pearson_matrix) <- all_expected_method_names
    
    for (i in 1:n_actual_methods) {
      for (j in 1:n_actual_methods) {
        m1_name <- all_expected_method_names[i]
        m2_name <- all_expected_method_names[j]
        
        if (i == j) {
          pearson_matrix[i, j] <- 1
        } else {
          vec1 <- numeric_values_by_method[[m1_name]]
          vec2 <- numeric_values_by_method[[m2_name]]
          
          # Check for variance to calculate correlation
          if (sd(vec1) == 0 || sd(vec2) == 0) {
            pearson_val <- 0 # Or NA, if preferred. 0 is a conservative approach if one or both are constant.
            cat(paste0("    DEBUG: Pearson for '", m1_name, "' vs '", m2_name, "' in '", ontology_name_for_plot, "' skipped due to zero variance. Setting to 0.\n"))
          } else {
            pearson_val <- cor(vec1, vec2, method = "pearson")
          }
          pearson_matrix[i, j] <- pearson_val
          
          # Collect data for the tibble
          if (j > i) {
            comp_type <- get_comparison_type(m1_name, m2_name, methods_FC_list, methods_Bayes_list)
            all_metrics_data_for_current_work_direction <- all_metrics_data_for_current_work_direction %>%
              add_row(
                Work = work_name,
                Ontology = ontology_name_for_plot,
                Direction = direction_label,
                Metric_Type = "Pearson",
                Value = pearson_val,
                Transformed_Value = pearson_val, # Pearson is already on a -1 to 1 scale
                Comparison_Type = comp_type
              )
          }
        }
      }
    }
    cat(paste("Pearson Correlation Matrix for ontology:", ontology_name_for_plot, " (", direction_label, ") - ", work_name, ":\n"))
    print(round(pearson_matrix, 3))
    
    # --- Calculate and Store Spearman Correlation ---
    spearman_matrix <- matrix(NA, nrow = n_actual_methods, ncol = n_actual_methods)
    rownames(spearman_matrix) <- all_expected_method_names
    colnames(spearman_matrix) <- all_expected_method_names
    
    for (i in 1:n_actual_methods) {
      for (j in 1:n_actual_methods) {
        m1_name <- all_expected_method_names[i]
        m2_name <- all_expected_method_names[j]
        
        if (i == j) {
          spearman_matrix[i, j] <- 1
        } else {
          vec1 <- numeric_values_by_method[[m1_name]]
          vec2 <- numeric_values_by_method[[m2_name]]
          
          # Check for variance to calculate correlation
          if (sd(vec1) == 0 || sd(vec2) == 0) {
            spearman_val <- 0 # Or NA.
            cat(paste0("    DEBUG: Spearman for '", m1_name, "' vs '", m2_name, "' in '", ontology_name_for_plot, "' skipped due to zero variance. Setting to 0.\n"))
          } else {
            spearman_val <- cor(vec1, vec2, method = "spearman")
          }
          spearman_matrix[i, j] <- spearman_val
          
          # Collect data for the tibble
          if (j > i) {
            comp_type <- get_comparison_type(m1_name, m2_name, methods_FC_list, methods_Bayes_list)
            all_metrics_data_for_current_work_direction <- all_metrics_data_for_current_work_direction %>%
              add_row(
                Work = work_name,
                Ontology = ontology_name_for_plot,
                Direction = direction_label,
                Metric_Type = "Spearman",
                Value = spearman_val,
                Transformed_Value = spearman_val, # Spearman is already on a -1 to 1 scale
                Comparison_Type = comp_type
              )
          }
        }
      }
    }
    cat(paste("Spearman Correlation Matrix for ontology:", ontology_name_for_plot, " (", direction_label, ") - ", work_name, ":\n"))
    print(round(spearman_matrix, 3))
    
    # --- Calculate and Store Euclidean Similarity ---
    euclidean_sim_matrix <- matrix(NA, nrow = n_actual_methods, ncol = n_actual_methods)
    rownames(euclidean_sim_matrix) <- all_expected_method_names
    colnames(euclidean_sim_matrix) <- all_expected_method_names
    
    for (i in 1:n_actual_methods) {
      for (j in 1:n_actual_methods) {
        m1_name <- all_expected_method_names[i]
        m2_name <- all_expected_method_names[j]
        
        if (i == j) {
          euclidean_sim_matrix[i, j] <- 1 # Maximum similarity with itself
        } else {
          vec1 <- numeric_values_by_method[[m1_name]]
          vec2 <- numeric_values_by_method[[m2_name]]
          
          euclidean_sim_val <- calculate_euclidean_similarity(vec1, vec2)
          euclidean_sim_matrix[i, j] <- euclidean_sim_val
          
          # Collect data for the tibble
          if (j > i) {
            comp_type <- get_comparison_type(m1_name, m2_name, methods_FC_list, methods_Bayes_list)
            all_metrics_data_for_current_work_direction <- all_metrics_data_for_current_work_direction %>%
              add_row(
                Work = work_name,
                Ontology = ontology_name_for_plot,
                Direction = direction_label,
                Metric_Type = "Euclidean_Similarity",
                Value = euclidean_sim_val,
                Transformed_Value = euclidean_sim_val, # Already scaled 0-1
                Comparison_Type = comp_type
              )
          }
        }
      }
    }
    cat(paste("Euclidean Similarity Matrix for ontology:", ontology_name_for_plot, " (", direction_label, ") - ", work_name, ":\n"))
    print(round(euclidean_sim_matrix, 3))
    
    # --- Generate Heatmaps (one for each metric) ---
    heatmap_metrics <- list(
      Jaccard_Index = jaccard_matrix,
      Pearson_Correlation = pearson_matrix,
      Spearman_Correlation = spearman_matrix,
      Euclidean_Similarity = euclidean_sim_matrix
    )
    
    for (metric_name in names(heatmap_metrics)) {
      mat_to_plot <- heatmap_metrics[[metric_name]]
      heatmap_path <- file.path(output_folder_current_work_path,
                                paste0(work_name, "_heatmap_", metric_name, "_",
                                       ontology_name_for_file, "_", direction_label, ".png"))
      
      png(heatmap_path, width = 1000, height = 1000, res = 150)
      
      # Adjust colors and breaks based on the metric
      if (metric_name %in% c("Jaccard_Index", "Euclidean_Similarity")) {
        # These metrics range from 0 to 1
        colors_pheatmap <- colorRampPalette(c("white", "steelblue", "darkblue"))(100)
        breaks_pheatmap <- seq(0, 1, length.out = 101)
      } else { # Pearson and Spearman range from -1 to 1
        colors_pheatmap <- colorRampPalette(c("firebrick4", "firebrick1", "white", "steelblue", "darkblue"))(100)
        breaks_pheatmap <- seq(-1, 1, length.out = 101)
      }
      
      pheatmap(mat_to_plot,
               clustering_distance_rows = "euclidean", # Still use euclidean for clustering
               clustering_distance_cols = "euclidean",
               clustering_method = "complete",
               main = paste(metric_name, " -", ontology_name_for_plot, " (", direction_label, ") - ", work_name),
               display_numbers = TRUE,
               fontsize_row = 10, fontsize_col = 10,
               color = colors_pheatmap,
               breaks = breaks_pheatmap,
               na_col = "lightgrey"
      )
      dev.off()
      cat(paste("Heatmap of", metric_name, " for", ontology_name_for_plot, " (", direction_label, ") saved at:", heatmap_path, "\n"))
    }
    
  } # End of ontology loop
  
  # --- STATISTICAL ANALYSIS AND BOXPLOTS FOR THE CURRENT WORK (all ontologies combined for this direction) ---
  cat(paste0("\n--- INITIATING STATISTICAL ANALYSIS OF METRICS FOR ", work_name, " (", direction_label, ") ---\n"))
  
  if (nrow(all_metrics_data_for_current_work_direction) == 0) {
    cat(paste0("\nWARNING: No valid metric data collected for ", work_name, " (", direction_label, "). Statistical analysis cannot proceed for this direction.\n"))
    return(tibble())
  }
  
  # Iterate through each metric type to generate box plots and perform tests
  for (current_metric_type in unique(all_metrics_data_for_current_work_direction$Metric_Type)) {
    cat(paste0("\n--- Processing metric: ", current_metric_type, " ---\n"))
    
    data_for_metric <- all_metrics_data_for_current_work_direction %>%
      filter(Metric_Type == current_metric_type)
    
    if (nrow(data_for_metric) == 0 || length(unique(data_for_metric$Comparison_Type)) < 2) {
      cat(paste0("WARNING: Insufficient data or comparison types for metric ", current_metric_type, " for ", work_name, " (", direction_label, "). Skipping analysis.\n"))
      next
    }
    
    # Summary of collected data
    cat(paste0("\nSummary of ", current_metric_type, " by comparison type (", work_name, ", ", direction_label, "): \n"))
    print(data_for_metric %>%
            group_by(Comparison_Type) %>%
            summarize(Median_Value = median(Transformed_Value, na.rm = TRUE),
                      Mean_Value = mean(Transformed_Value, na.rm = TRUE),
                      Count = n()))
    
    # Perform Kruskal-Wallis test (if more than one comparison type and at least 2 data points per group)
    # Filter groups with less than 2 observations to avoid errors in kruskal.test and dunn.test
    data_for_kruskal <- data_for_metric %>%
      group_by(Comparison_Type) %>%
      filter(n() >= 2) %>%
      ungroup()
    
    # --- Robust handling of Kruskal-Wallis test ---
    if (length(unique(data_for_kruskal$Comparison_Type)) > 1 &&
        # Add a check to ensure sufficient variability
        length(unique(data_for_kruskal$Transformed_Value)) > 1) { 
      
      cat(paste0("\nPerforming Kruskal-Wallis Test for ", current_metric_type, " (", work_name, ", ", direction_label, "):\n"))
      kruskal_result <- kruskal.test(Transformed_Value ~ Comparison_Type, data = data_for_kruskal)
      
      # Path to save Kruskal-Wallis test results
      kruskal_txt_path <- file.path(output_folder_current_work_path,
                                    paste0(work_name, "_kruskal_test_results_", current_metric_type, "_", direction_label, ".txt"))
      
      # Save the test result
      capture.output(print(kruskal_result), file = kruskal_txt_path)
      cat(paste("Kruskal-Wallis test results saved at:", kruskal_txt_path, "\n"))
      
      if (is.na(kruskal_result$p.value)) {
        # If p-value is NA, the test could not be performed correctly (e.g., constant data)
        message(paste0("INFO: Kruskal-Wallis test for ", work_name, " (", direction_label, ") and metric ", current_metric_type, " resulted in an NA p-value. This usually happens if the data for this metric are constant (e.g., all zeros). Skipping post-hoc tests."))
        
        # Optionally: Also add a note to the test results file
        cat("\n--- WARNING: Kruskal-Wallis test could not compute a p-value (NaN/NA). Possibly, metric values are constant or lack variability. No post-hoc tests will be performed.---\n", 
            file = kruskal_txt_path, append = TRUE)
        
      } else if (kruskal_result$p.value < 0.05) {
        # If p-value is significant, perform post-hoc tests (e.g., Dunn's test)
        message(paste0("INFO: Kruskal-Wallis test for ", work_name, " (", direction_label, ") and metric ", current_metric_type, " is significant (p < 0.05). Performing post-hoc tests."))
        
        # Perform Dunn's Test (post-hoc)
        # Add tryCatch() for Dunn's test as well, in case it fails.
        tryCatch({
          dunn_result <- dunn.test(x = data_for_kruskal$Transformed_Value,
                                   g = data_for_kruskal$Comparison_Type,
                                   method = "bonferroni",
                                   altp = TRUE)
          print(dunn_result)
          
          dunn_txt_path <- file.path(output_folder_current_work_path,
                                     paste0(work_name, "_dunn_test_results_", current_metric_type, "_", direction_label, ".txt"))
          capture.output(print(dunn_result), file = dunn_txt_path)
          cat(paste("Dunn's test results saved at:", dunn_txt_path, "\n"))
        }, error = function(e) {
          warning(paste0("WARNING: Error performing Dunn Test for ", work_name, " (", direction_label, ") and metric ", current_metric_type, ": ", e$message))
          cat(paste0("\n--- WARNING: Error performing Dunn Test: ", e$message, "---\n"), 
              file = kruskal_txt_path, append = TRUE) # Add to the same Kruskal-Wallis file
        })
        
      } else {
        # If p-value is not significant
        message(paste0("INFO: Kruskal-Wallis test for ", work_name, " (", direction_label, ") and metric ", current_metric_type, " is not significant (p >= 0.05). No post-hoc tests required."))
        # Add a note to the results file
        cat("\n--- Kruskal-Wallis test was not significant (p >= 0.05). No post-hoc tests will be performed.---\n", 
            file = kruskal_txt_path, append = TRUE)
      }
    } else {
      cat(paste0("\nINFO: Insufficient valid groups (at least 2 observations and variability) to perform Kruskal-Wallis test for ", current_metric_type, " (", work_name, ", ", direction_label, "). Skipping.\n"))
      # Optional: Create a specific log file to indicate why the test was not performed.
      kruskal_txt_path <- file.path(output_folder_current_work_path,
                                    paste0(work_name, "_kruskal_test_results_", current_metric_type, "_", direction_label, ".txt"))
      cat(paste0("INFO: Kruskal-Wallis test was not performed for this metric due to insufficient data or lack of variability. (Work: ", work_name, ", Direction: ", direction_label, ", Metric: ", current_metric_type, ")\n"), 
          file = kruskal_txt_path)
    }
    # --- End of Kruskal-Wallis test robust handling ---
    
    # --- Visualization with Box Plots ---
    cat(paste0("\nGenerating Box Plots for ", current_metric_type, " (", work_name, ", ", direction_label, ")...\n"))
    
    common_theme_work_plots <- theme_minimal(base_size = 10) +
      theme(
        plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
        axis.title = element_text(size = 10),
        panel.grid = element_blank(),
        axis.line.y = element_line(colour = "black"),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        panel.background = element_rect(fill = "white", colour = "black"),
        plot.background = element_rect(fill = "white", colour = NA)
      )
    
    # Define Y-axis label based on metric
    y_axis_label <- switch(current_metric_type,
                           "Jaccard" = "Jaccard Index (arcsin(sqrt))",
                           "Pearson" = "Pearson Correlation Coefficient",
                           "Spearman" = "Spearman Correlation Coefficient",
                           "Euclidean_Similarity" = "Euclidean Similarity (1/(1+d))",
                           "Value") # Default
    
    # Define dynamic comparisons for ggpubr based on the 4 requested categories
    my_comparisons_for_boxplot <- list(
      c("Intra-HTM_FC_CBR", "Intra-HTM_Bayes_CBR"),
      c("Intra-HTM_FC_CBR", "Intra-CBR_Fixed_HTM"),
      c("Intra-HTM_FC_CBR", "Inter-HTM_Inter-CBR"),
      c("Intra-HTM_Bayes_CBR", "Intra-CBR_Fixed_HTM"),
      c("Intra-HTM_Bayes_CBR", "Inter-HTM_Inter-CBR"),
      c("Intra-CBR_Fixed_HTM", "Inter-HTM_Inter-CBR")
    )
    
    # Filter comparisons to ensure both groups exist in the data
    valid_comparisons <- list()
    for (comp in my_comparisons_for_boxplot) {
      if (all(comp %in% unique(data_for_metric$Comparison_Type))) {
        valid_comparisons <- c(valid_comparisons, list(comp))
      }
    }
    
    plot_all_ontologies_current_work_direction <- ggboxplot(data_for_metric, x = "Comparison_Type", y = "Transformed_Value",
                                                            color = "Comparison_Type", palette = "jco",
                                                            add = "jitter",
                                                            ylab = y_axis_label, xlab = "Comparison Type",
                                                            title = paste0(current_metric_type, " Distribution (", work_name, " - ", direction_label, " - All Ontologies)")) +
      stat_compare_means(method = "kruskal.test", label.y = max(data_for_metric$Transformed_Value, na.rm = TRUE) * 1.1, size = 3) +
      common_theme_work_plots +
      theme(legend.position = "none")
    
    if(length(valid_comparisons) > 0) {
      plot_all_ontologies_current_work_direction <- plot_all_ontologies_current_work_direction +
        stat_compare_means(comparisons = valid_comparisons,
                           label = "p.signif", method = "wilcox.test", size = 3) # wilcox.test for non-parametric post-hoc
    }
    
    boxplot_all_ontologies_current_work_direction_path <- file.path(output_folder_current_work_path,
                                                                    paste0(work_name, "_boxplot_all_ontologies_", current_metric_type, "_", direction_label, ".png"))
    ggsave(boxplot_all_ontologies_current_work_direction_path, plot = plot_all_ontologies_current_work_direction, width = 10, height = 7, dpi = 300)
    cat(paste("Box plot for ", work_name, " (", direction_label, ", all ontologies) for ", current_metric_type, " saved at:", boxplot_all_ontologies_current_work_direction_path, "\n"))
    
    # You can uncomment this if you need individual box plots per ontology (may generate many files)
    # for (ontology_name_clean in unique(data_for_metric$Ontology)) {
    #   data_ontology <- data_for_metric %>% filter(Ontology == ontology_name_clean)
    #   
    #   if (nrow(data_ontology) > 0 && length(unique(data_ontology$Comparison_Type)) > 1) {
    #     valid_comparisons_ontology <- list()
    #     for (comp in my_comparisons_for_boxplot) { # Use the same base comparison list
    #       if (all(comp %in% unique(data_ontology$Comparison_Type))) {
    #         valid_comparisons_ontology <- c(valid_comparisons_ontology, list(comp))
    #       }
    #     }
    #     
    #     plot_ontology <- ggboxplot(data_ontology, x = "Comparison_Type", y = "Transformed_Value",
    #                                 color = "Comparison_Type", palette = "jco",
    #                                 add = "jitter",
    #                                 ylab = y_axis_label, xlab = "Comparison Type",
    #                                 title = paste(current_metric_type, " (", work_name, " - ", direction_label, " - ", ontology_name_clean, ")")) +
    #       stat_compare_means(method = "kruskal.test", label.y = max(data_ontology$Transformed_Value, na.rm = TRUE) * 1.1, size = 3) +
    #       common_theme_work_plots +
    #       theme(legend.position = "none")
    #     
    #     if(length(valid_comparisons_ontology) > 0) {
    #       plot_ontology <- plot_ontology +
    #         stat_compare_means(comparisons = valid_comparisons_ontology,
    #                            label = "p.signif", method = "wilcox.test", size = 3)
    #     }
    #     
    #     boxplot_ontology_path <- file.path(output_folder_current_work_path,
    #                                         paste0(work_name, "_boxplot_", current_metric_type, "_", gsub("[^[:alnum:]]", "_", ontology_name_clean), "_", direction_label, ".png"))
    #     ggsave(boxplot_ontology_path, plot = plot_ontology, width = 8, height = 6, dpi = 300)
    #     cat(paste("Box plot for ", work_name, " - ", ontology_name_clean, " (", direction_label, ") for ", current_metric_type, " saved at:", boxplot_ontology_path, "\n"))
    #   } else {
    #     cat(paste("INFO: Insufficient data or comparison types to generate a box plot for ", work_name, " - ", ontology_name_clean, " (", direction_label, ") for ", current_metric_type, ". Skipping.\n"))
    #   }
    # }
  } # End of metric loop
  
  return(all_metrics_data_for_current_work_direction)
}


# --- 4. Main Execution Block ---

# Create output folders
if (!dir.exists(consolidated_tsvs_folder_path)) {
  dir.create(consolidated_tsvs_folder_path, recursive = TRUE)
  cat(paste("Folder for consolidated TSVs created at:", consolidated_tsvs_folder_path, "\n"))
}
if (!dir.exists(output_results_folder_path)) {
  dir.create(output_results_folder_path, recursive = TRUE)
  cat(paste("Analysis results folder created at:", output_results_folder_path, "\n"))
}

# --- Step 1: Generate consolidated TSV files from RAW .xls files ---
# Call the consolidation function that iterates through all works and directions
consolidate_cluego_results(
  base_input_dir = cluego_raw_folder_path,
  output_tsv_dir = consolidated_tsvs_folder_path,
  htms = htms,
  cbrs = cbrs,
  all_expected_method_names = all_expected_method_names,
  works_list = works_list
)

# --- Step 2: Prepare the list of consolidated TSV files for analysis ---
# This list is now built based on what the consolidation function should have created.
input_files_for_analysis <- list()
for (work_name in works_list) {
  for (direction_label in c("up", "down")) {
    tsv_filename <- paste0(work_name, "_0_Enrich_comp_results_", direction_label, "_all_methods.tsv")
    tsv_filepath <- file.path(consolidated_tsvs_folder_path, tsv_filename)
    
    # Only add to the analysis list if the file is expected to exist after consolidation
    # (the consolidation function already handles if the original .xls did not exist, generating a TSV with 0s)
    input_files_for_analysis <- c(input_files_for_analysis, list(list(
      path = tsv_filepath,
      work = work_name,
      direction = direction_label
    )))
  }
}


# --- Step 3: Execute analysis on consolidated TSVs ---

# Initialize a global tibble to collect all Jaccard and Correlation data
# from all works and directions
all_jaccard_and_correlation_data <- tibble(
  Work = character(),
  Ontology = character(),
  Direction = character(),
  Metric_Type = character(),
  Value = numeric(),
  Transformed_Value = numeric(),
  Comparison_Type = character()
)

# Loop through each input file for analysis (now all generated files)
for (file_info in input_files_for_analysis) {
  current_tsv_path <- file_info$path
  current_work_name <- file_info$work
  current_direction <- file_info$direction
  
  results_for_current_work <- process_consolidated_tsv_for_work_analysis(
    tsv_file_path = current_tsv_path,
    work_name = current_work_name,
    direction_label = current_direction,
    output_base_dir = output_results_folder_path, # Use the analysis output path
    all_expected_method_names = all_expected_method_names,
    methods_FC_list = methods_FC,
    methods_Bayes_list = methods_Bayes
  )
  
  # Combine results from this work/direction with the global tibble
  all_jaccard_and_correlation_data <- bind_rows(all_jaccard_and_correlation_data, results_for_current_work)
}

cat("\n--- All file processing completed. ---\n")
cat(paste("Global metric data collected (rows):", nrow(all_jaccard_and_correlation_data), "\n"))

# --- Global analysis and visualization combining all works and directions ---
# This block is useful if you want a summary of YOUR ENTIRE dataset, not just by work/direction.
# You can uncomment it if you have multiple TSV files and want a global analysis.

if (nrow(all_jaccard_and_correlation_data) > 0) {
  cat("\n--- Combined Analysis and Box Plots for ALL works and directions ---\n")
  
  # Iterate through each metric type to generate box plots and perform tests
  for (metric_type_global in unique(all_jaccard_and_correlation_data$Metric_Type)) {
    cat(paste0("\n--- Processing GLOBAL metric: ", metric_type_global, " ---\n"))
    
    data_for_global_metric <- all_jaccard_and_correlation_data %>%
      filter(Metric_Type == metric_type_global)
    
    # Ensure there's enough data and comparison types for analysis
    data_for_global_kruskal <- data_for_global_metric %>%
      group_by(Comparison_Type) %>%
      filter(n() >= 2) %>%
      ungroup()
    
    if (nrow(data_for_global_kruskal) == 0 || length(unique(data_for_global_kruskal$Comparison_Type)) < 2) {
      cat(paste0("WARNING: Insufficient data or comparison types for global metric ", metric_type_global, ". Skipping global analysis.\n"))
      next
    }
    
    # Global summary
    cat(paste0("\nGlobal Summary of ", metric_type_global, " by comparison type (All Works/Directions): \n"))
    print(data_for_global_metric %>%
            group_by(Comparison_Type) %>%
            summarize(Median_Value = median(Transformed_Value, na.rm = TRUE),
                      Mean_Value = mean(Transformed_Value, na.rm = TRUE),
                      Count = n()))
    
    # Global Kruskal-Wallis
    # --- Robust handling of GLOBAL Kruskal-Wallis test ---
    if (length(unique(data_for_global_kruskal$Comparison_Type)) > 1 &&
        length(unique(data_for_global_kruskal$Transformed_Value)) > 1) {
      
      cat(paste0("\nPerforming GLOBAL Kruskal-Wallis Test for ", metric_type_global, ":\n"))
      kruskal_result_global <- kruskal.test(Transformed_Value ~ Comparison_Type, data = data_for_global_kruskal)
      
      global_kruskal_txt_path <- file.path(output_results_folder_path,
                                           paste0("Global_kruskal_test_results_", metric_type_global, ".txt"))
      capture.output(print(kruskal_result_global), file = global_kruskal_txt_path)
      cat(paste("GLOBAL Kruskal-Wallis test results saved at:", global_kruskal_txt_path, "\n"))
      
      if (is.na(kruskal_result_global$p.value)) {
        message(paste0("INFO: GLOBAL Kruskal-Wallis test for metric ", metric_type_global, " resulted in an NA p-value. This usually happens if the data for this metric are constant (e.g., all zeros). Skipping post-hoc tests."))
        cat("\n--- WARNING: GLOBAL Kruskal-Wallis test could not compute a p-value (NaN/NA). Possibly, metric values are constant or lack variability. No post-hoc tests will be performed.---\n", 
            file = global_kruskal_txt_path, append = TRUE)
      } else if (kruskal_result_global$p.value < 0.05) {
        cat("\nGLOBAL Kruskal-Wallis test is significant (p < 0.05). Performing Dunn's post-hoc test:\n")
        
        tryCatch({
          dunn_result_global <- dunn.test(x = data_for_global_kruskal$Transformed_Value,
                                          g = data_for_global_kruskal$Comparison_Type,
                                          method = "bonferroni",
                                          altp = TRUE)
          print(dunn_result_global)
          
          global_dunn_txt_path <- file.path(output_results_folder_path,
                                            paste0("Global_dunn_test_results_", metric_type_global, ".txt"))
          capture.output(print(dunn_result_global), file = global_dunn_txt_path)
          cat(paste("GLOBAL Dunn's test results saved at:", global_dunn_txt_path, "\n"))
        }, error = function(e) {
          warning(paste0("WARNING: Error performing GLOBAL Dunn Test for metric ", metric_type_global, ": ", e$message))
          cat(paste0("\n--- WARNING: Error performing GLOBAL Dunn Test: ", e$message, "---\n"), 
              file = global_kruskal_txt_path, append = TRUE)
        })
      } else {
        cat("\nGLOBAL Kruskal-Wallis test was not significant (p >= 0.05). Dunn's post-hoc test is not needed.\n")
        cat("\n--- GLOBAL Kruskal-Wallis test was not significant (p >= 0.05). No post-hoc tests will be performed.---\n", 
            file = global_kruskal_txt_path, append = TRUE)
      }
    } else {
      cat(paste0("\nINFO: Insufficient valid groups (at least 2 observations and variability) to perform GLOBAL Kruskal-Wallis test for ", metric_type_global, ". Skipping.\n"))
      global_kruskal_txt_path <- file.path(output_results_folder_path,
                                           paste0("Global_kruskal_test_results_", metric_type_global, ".txt"))
      cat(paste0("INFO: GLOBAL Kruskal-Wallis test was not performed for this metric due to insufficient data or lack of variability. (Metric: ", metric_type_global, ")\n"), 
          file = global_kruskal_txt_path)
    }
    # --- End of GLOBAL Kruskal-Wallis robust handling ---
    
    # Global Box Plot
    y_axis_label_global <- switch(metric_type_global,
                                  "Jaccard" = "Jaccard Index (arcsin(sqrt))",
                                  "Pearson" = "Pearson Correlation Coefficient",
                                  "Spearman" = "Spearman Correlation Coefficient",
                                  "Euclidean_Similarity" = "Euclidean Similarity (1/(1+d))",
                                  "Value")
    
    # Define comparisons for ggpubr for the global plot (the same 6 comparisons)
    my_comparisons_for_boxplot_global <- list(
      c("Intra-HTM_FC_CBR", "Intra-HTM_Bayes_CBR"),
      c("Intra-HTM_FC_CBR", "Intra-CBR_Fixed_HTM"),
      c("Intra-HTM_FC_CBR", "Inter-HTM_Inter-CBR"),
      c("Intra-HTM_Bayes_CBR", "Intra-CBR_Fixed_HTM"),
      c("Intra-HTM_Bayes_CBR", "Inter-HTM_Inter-CBR"),
      c("Intra-CBR_Fixed_HTM", "Inter-HTM_Inter-CBR")
    )
    
    # Filter comparisons to ensure both groups exist in the global data
    valid_comparisons_global <- list()
    for (comp in my_comparisons_for_boxplot_global) {
      if (all(comp %in% unique(data_for_global_metric$Comparison_Type))) {
        valid_comparisons_global <- c(valid_comparisons_global, list(comp))
      }
    }
    
    plot_global_metric <- ggboxplot(data_for_global_metric, x = "Comparison_Type", y = "Transformed_Value",
                                    color = "Comparison_Type", palette = "jco",
                                    add = "jitter",
                                    ylab = y_axis_label_global, xlab = "Comparison Type",
                                    title = paste0("Global ", metric_type_global, " Distribution (All Works/Directions)")) +
      stat_compare_means(method = "kruskal.test", label.y = max(data_for_global_metric$Transformed_Value, na.rm = TRUE) * 1.1, size = 3) +
      theme_minimal(base_size = 10) + # Use minimal theme for global plots as well
      theme(
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        panel.grid = element_blank(),
        axis.line.y = element_line(colour = "black"),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        panel.background = element_rect(fill = "white", colour = "black"),
        plot.background = element_rect(fill = "white", colour = NA)
      ) +
      theme(legend.position = "none") # Remove color legend if already on X-axis
    
    if(length(valid_comparisons_global) > 0) {
      plot_global_metric <- plot_global_metric +
        stat_compare_means(comparisons = valid_comparisons_global,
                           label = "p.signif", method = "wilcox.test", size = 3) # wilcox.test for non-parametric post-hoc
    }
    
    global_boxplot_path <- file.path(output_results_folder_path,
                                     paste0("Global_boxplot_all_metrics_", metric_type_global, ".png"))
    ggsave(global_boxplot_path, plot = plot_global_metric, width = 10, height = 7, dpi = 300)
    cat(paste("GLOBAL Box plot for", metric_type_global, " saved at:", global_boxplot_path, "\n"))
  }
}